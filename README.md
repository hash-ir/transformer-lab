# transformer-lab ðŸ¤–
A WIP project featuring implementation of GPT and related transformer
models from scratch. Inspired by Andrej Karpathy's famous "Let's Build GPT" [tutorial](https://youtu.be/kCc8FmEb1nY?si=yd_MHutBE82WviUe). This repository is actively evolving. 

## Features âœ¨
- Decoder implementation of the [Transformer](https://arxiv.org/pdf/1706.03762)
    * Multi-head self-attention (parallel processing)
    * The Transformer block: connection followed by computation
    * Text generator based on a context
- Byte Pair Encoding (BPE) algorithm for tokenization, popularized by the [GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

## Roadmap ðŸŽ¯
- Custom dataset training
- Pretraining and fine-tuning experiments
- Implementation of "Encoder" block

Contributions and feedback are welcome!