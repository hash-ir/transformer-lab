# transformer-lab ðŸ¤–
A WIP project featuring implementation of GPT and related transformer
models from scratch. Inspired by Andrej Karpathy's famous "Let's Build GPT" [tutorial](https://youtu.be/kCc8FmEb1nY?si=yd_MHutBE82WviUe). This repository is actively evolving. 

## Features âœ¨
- Decoder implementation of the [Transformer](https://arxiv.org/pdf/1706.03762)
    * Single- and multi-head self-attention
    * The Transformer block: connection followed by computation
    * Text generator based on a context

## Roadmap ðŸŽ¯
- Parallel processing of attention heads
- Custom dataset training
- Pretraining and fine-tuning experiments
- Implementation of "Encoder" block

Contributions and feedback are welcome!